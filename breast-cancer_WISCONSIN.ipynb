{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p  style=\"text-align: center;\"><font size=\"10\"><b>PREDICTING BREAST CANCER IN WISCONSIN</b></font></p>\n\n\nUsing data from a digitized images of a brest mass in the state of Wisconsin, this notebook will use feature selection and model building using several different algorithms to attempt to predict whether a breast mass is benign or malignant. "},{"metadata":{},"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents</h3>\n\n1. [Libraries & Packages](#libraries)\n2. [Initial Insights](#insights)\n3. [Data Preprocessing & Feature Engineering](#preprocessing)\n4. [Data Exploration & Visualization](#exploration)\n5. [Feature Selection](#features)  \n6. [Model Building](#models)  \n    A. [Random Forest](#rf)  \n    B. [Random Forest w Select K Best](#rfkbest)  \n    C. [Support Vector Machine](#svm)  \n    D. [Logistic Regression](#lr)  \n    E. [Decision Tree](#dt)  \n    F. [K Nearest Neighbors](#knn) \n7. [Algorithm Comparison](#comparison)\n8. [Conclusion](#conclusion) "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n## LIBRARIES & PACKAGES "},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade scikit-learn==0.20.3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nimport matplotlib.image as mpimg\n%matplotlib inline \n\nimport itertools\n\n\n#EVALUATION ALGORITHMS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn import metrics\n# from sklearn.metrics import jaccard_score\nfrom sklearn.externals.six import StringIO\nfrom sklearn import tree\n\nimport pydotplus\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"insights\"></a>\n## INITIAL INSIGHTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"preprocesing\"></a>\n## DATA PRE-PROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHAPE OF FEATURE DATASET\n\ndf.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\nY = df.diagnosis\nX = df.drop('diagnosis', axis=1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DATA STANDARDIZATION\nX_std = (X - X.mean()) / (X.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"exploration\"></a>\n## DATA EXPLORATION & VISUALIZATION\n\nThis section will use visualization techniques to get an overview of the data and the correlation between each feature and the target variable. \n\nUsing violin and swarm plots we'll be able to see what kind of distinctions there are between benign and malignant cases and their respective feature variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#VISUALIZE NUMBER OF BENIGN AND MALIGNANT CASES\n\nsns.countplot(df['diagnosis'], label='Count')\n\nB, M = df['diagnosis'].value_counts()\nprint('Benign: ',B)\nprint('Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SPLIT DATASET INTO TWO SETS OF 15 FEATURES EACH\n\ndf_set1 = pd.concat([Y, X_std.iloc[:, 0:15]], axis=1)\ndf_set2 = pd.concat([Y, X_std.iloc[:, 15:30]], axis=1)\n\n\n# TRANSFORM DATA INTO 3 COLUMN DATA FRAME W/ ALL FEATURES IN ONE COLUMN\n\ndf_melt1 = pd.melt(df_set1, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\ndf_melt2 = pd.melt(df_set2, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\ndf_melt1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIOLIN PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nplt.figure(figsize=(15,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt1, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWARM PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nsns.set(style='whitegrid', palette='muted')\n\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt1)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIOLIN PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nplt.figure(figsize=(15,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt2, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWARM PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nsns.set(style='whitegrid', palette='muted')\n\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt2)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"features\"></a>\n## FEATURE SELECTION\n\nWe will narrow down our features by using a heatmap to visualize the correlation between variables and eliminating those features that are fully correlated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE HEATMAP TO VISUALIZE DATA CORRELATIONS\n\nplt.figure(figsize=(16,16))\nsns.heatmap(df.corr(), cbar = True,  square = True, annot=True, fmt= '.1f', annot_kws={'size': 12},\n           xticklabels=X.columns, yticklabels=X.columns,\n           cmap= 'YlGnBu')\nplt.title('FEATURE VARIABLE CORRELATIONS')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several features have a 100% correlation. For instance, **radius_mean**, **perimeter_mean**, and **area_mean** are all 100% correlated so we can keep one and eliminate the rest. We'll keep **area_mean**. \n\nThis step will be repeated for the other features until we have a feature set that is narrowed down to the most essential features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE NEW FEATURE SET \n\nfeatures = ['area_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean',\n        'area_se', 'texture_se', 'smoothness_se', 'compactness_se', 'symmetry_se', 'fractal_dimension_se',\n        'area_worst', 'texture_worst', 'smoothness_worst', 'compactness_worst', 'symmetry_worst', 'fractal_dimension_worst']\nX_1 = X[features]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"models\"></a>\n## MODEL BUILDING\n\nUsing our new feature set we will build several models to determine which method is best for predicting the outcome of our target variable, diagnosis. \n\nWe will then evaluate the accuracy of each model using **accuracy_score**, **F1_Score**, **Confusion Matrix**, and **Log Loss**\n\n### MODELS USED:\n1. RANDOM FOREST\n2. RANDOM FOREST USING SELECT K BEST FEATURES\n3. SUPPORT VECTOR MACHINE\n4. LOGISTIC REGRESSION\n5. DECISION TREE\n6. K NEAREST NEIGHBOR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rf\"></a>\n### RANDOM FOREST CLASSIFICATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_1, Y, test_size=0.3, random_state=1)\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nyhat_rf = clf_rf.predict(X_test)\nyhat_proba_rf = clf_rf.predict_proba(X_test)\n\nac_rf = accuracy_score(y_test, yhat_rf)\nprint('Accuracy Score: ', ac_rf)\n\ncm_rf = confusion_matrix(y_test, yhat_rf)\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Random Forest')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1 SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_rf = f1_score(y_test, yhat_rf, average='weighted') \nprint('F1 Score: ', f1_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_rf = log_loss(y_test, yhat_proba_rf)\nlog_loss_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rfkbest\"></a>\n### SELECT K BEST AND RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_features = SelectKBest(chi2, k=9).fit(X_train, y_train)\nX_train.columns[select_features.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_scores = pd.DataFrame(X_train.columns, columns=['Features'])\nfeature_scores['scores'] = select_features.scores_\nfeature_scores = feature_scores.sort_values(by='scores', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_2 = select_features.transform(X_train)\nX_test_2 = select_features.transform(X_test)\n\nclf_rf2 = RandomForestClassifier()\nclf_rf2.fit(X_train_2, y_train)\nyhat_rf2 = clf_rf2.predict(X_test_2)\nyhat_proba_rf2 = clf_rf2.predict_proba(X_test_2)\n\nac_rf2 = accuracy_score(y_test, yhat_rf2)\nprint('Accuracy Score: ', ac_rf2)\n\ncm_rf_kbest = confusion_matrix(y_test, yhat_rf2)\nsns.heatmap(cm_rf_kbest, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Random Forest 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1_SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_rf2 = f1_score(y_test, yhat_rf2, average='weighted')\nprint('F1 Score: ', f1_rf2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LOG LOSS"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_rf2 = log_loss(y_test, yhat_proba_rf2)\nlog_loss_rf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"svm\"></a>\n### SUPPORT VECTOR MACHINE"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svm = svm.SVC(kernel='rbf', probability=True)\nclf_svm.fit(X_train, y_train)\nyhat_svm = clf_svm.predict(X_test)\nyhat_proba_svm = clf_svm.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_svm = accuracy_score(y_test, yhat_svm)\nprint('Accuracy Score: ', ac_svm)\n\ncm_svm = confusion_matrix(y_test, yhat_svm)\nsns.heatmap(cm_svm, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Support Vector Machine')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1_SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_svm = f1_score(y_test, yhat_svm, average='weighted')\nprint('F1 Score: ', f1_svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LOG LOSS"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_svm = log_loss(y_test, yhat_proba_svm)\nlog_loss_svm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr\"></a>\n### LOGISTIC REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_lr = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nclf_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_lr.fit(X_train, y_train)\nyhat_lr = clf_lr.predict(X_test)\nyhat_proba_lr = clf_lr.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_lr = accuracy_score(y_test, yhat_lr)\nprint('Accuracy Score: ', ac_lr)\n\ncm_lr = confusion_matrix(y_test, yhat_lr)\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1 SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_lr = f1_score(y_test, yhat_lr, average='weighted') \nprint('F1 Score: ', f1_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (classification_report(y_test, yhat_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LOG LOSS"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_lr = log_loss(y_test, yhat_proba_lr)\nlog_loss_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"dt\"></a>\n### DECISION TREE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nclf_dt.fit(X_train, y_train)\nyhat_dt = clf_dt.predict(X_test)\nyhat_proba_dt = clf_dt.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(yhat_dt, y_test)\nac_dt = metrics.accuracy_score(yhat_dt, y_test)\nprint(\"DecisionTrees's Accuracy: \", ac_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_dt = confusion_matrix(y_test, yhat_dt)\nsns.heatmap(cm_dt, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Decision Tree')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1 SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_dt = f1_score(y_test, yhat_dt, average='weighted') \nprint('F1 Score: ', f1_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LOG LOSS"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_dt = log_loss(y_test, yhat_proba_dt)\nlog_loss_dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### VISUALIZE DECISION TREE"},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = StringIO()\nfilename = \"clf_dt.png\"\nfeatureNames = X_train.columns[0:18]\ntargetNames = Y.unique().tolist()\nout=tree.export_graphviz(clf_dt,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_train), filled=True,  special_characters=True,rotate=False)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(100, 200))\nplt.imshow(img,interpolation='nearest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"knn\"></a>\n### K NEAREST NEIGHBORS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 7\nclf_knn = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat_knn = clf_knn.predict(X_test)\nyhat_proba_knn = clf_knn.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_knn = metrics.accuracy_score(y_test, yhat_knn)\n\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, clf_knn.predict(X_train)))\nprint(\"Test set Accuracy: \", ac_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n# ConfusionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    clf_knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat_knn=clf_knn.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat_knn)\n\n    \n    std_acc[n-1]=np.std(yhat_knn==y_test)/np.sqrt(yhat_knn.shape[0])\n\nmean_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( \"The best accuracy was with\", mean_acc.max(), \"with k =\", mean_acc.argmax()+1) \n\ncm_knn = confusion_matrix(y_test, yhat_knn)\nsns.heatmap(cm_knn, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - K Nearest Neighbors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1 SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_knn = f1_score(y_test, yhat_knn, average='weighted') \nprint('F1 Score: ', f1_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LOG LOSS"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss_knn = log_loss(y_test, yhat_proba_knn)\nlog_loss_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"comparison\"></a>\n## CLASSIFICATION ACCURACY COMPARISON\n\nWe will do a side by side comparison and a visualization of each algorithm's **accuracy_score**, **f1_score**, and **log loss** to determine which model yielded the best results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE NEW DATAFRAME WITH THE ALGORITHM AND EACH ACCURACY MEASUREMENT. \n\nd = {'Algorithm' : ['Random Forest', 'Random Forest w/ KBest', 'Support Vector Machine', 'Logistic Regression', 'Decision Tree', 'K Nearest Neighbor'],\n     'Accuracy_Score' : [ac_rf, ac_rf2, ac_svm, ac_lr, ac_dt, ac_knn],\n    'F1_Score' : [f1_rf, f1_rf2, f1_svm, f1_lr, f1_dt, f1_knn],\n    'Log_Loss' : [log_loss_rf, log_loss_rf2, log_loss_svm, log_loss_lr, log_loss_dt, log_loss_knn]}\ndf_accuracy = pd.DataFrame(data=d)\ndf_accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE BAR CHART TO VISUALIZE EACH ALGORITHM'S ACCURACY MEASUREMENT. \n\nfig = go.Figure(data=[go.Bar(name='Accuracy_Score', x=df_accuracy['Algorithm'], y=df_accuracy['Accuracy_Score']),\n                      go.Bar(name='F1_Score', x=df_accuracy['Algorithm'], y=df_accuracy['F1_Score']),\n                      go.Bar(name='Log_Loss', x=df_accuracy['Algorithm'], y=df_accuracy['Log_Loss']),\n                     ])\n\n# Change the bar mode\nfig.update_layout(barmode='group', title_text='Classification Scores')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE SUBPLOTS WITH ALL CONFUSIION MATRICES\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8), sharey=True)\nfig.suptitle(\"CONFUSION MATRICES\", fontsize=16)\nfig.text(0.5, 0.04, 'PREDICTED VALUES (YHAT)', ha='center', va='center')\nfig.text(0.06, 0.5, 'ACTUAL VALUES (Y)', ha='center', va='center', rotation='vertical')\n\nax1 = sns.heatmap(cm_rf, ax=axes[0, 0], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax2 = sns.heatmap(cm_rf_kbest,ax=axes[0, 1], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax3 = sns.heatmap(cm_svm, ax=axes[0, 2], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax4 = sns.heatmap(cm_lr, ax=axes[1, 0], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax5 = sns.heatmap(cm_dt, ax=axes[1, 1], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax6 = sns.heatmap(cm_knn, ax=axes[1, 2], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\n\nax1.set_title('Random Forest')\nax2.set_title('Random Forest - KBest')\nax3.set_title('Support Vector Machine')\nax4.set_title('Logistic Regression')\nax5.set_title('Decision Tree')\nax6.set_title('K Neareest Neighbors')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n## CONCLUSION\n\nIt appears that the Random Forest algorithm gives us the best chance at accuracy within our dataset with an accuracy score of 93% and Log Loss of 16%. \n\nThank you for stopping by! I'd love to recieve some feedback or suggestions on what I could do to improve this kernel. Please leave a comment below. \n\n-Milton"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}